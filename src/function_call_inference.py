#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
function_call_infer_lora.py  â€”  æ¨ç† (Base 4-bit + LoRA Î”æ¬Šé‡)
æ›´æ–°ï¼šä½¿ç”¨å®Œæ•´åŒ¹é…ä¾†åš´æ ¼åˆ¤å®š call èªå¥
"""

import argparse
import json
import re
import torch
from typing import Optional
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# --------------------------------------------------
# å›ºå®šç³»çµ±æç¤ºèˆ‡åš´æ ¼åŒ¹é…æ­£å‰‡
# --------------------------------------------------
SYSTEM_PROMPT = (
    "ä½ æœ‰ä¸€å€‹å·¥å…· generate_image({prompt:str} â†’ URL)ã€‚"
    "åªæœ‰ç•¶ä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ç”¢ç”Ÿåœ–ç‰‡æ™‚æ‰å‘¼å«ï¼›"
    "å¦å‰‡è«‹ç”¨è‡ªç„¶èªè¨€å›ç­”ã€‚"
)
# å®Œæ•´åŒ¹é… callèªå¥ï¼Œé¿å…ä»»ä½•é¡å¤–æ–‡å­—
CALL_FULLMATCH = re.compile(
    r'^\s*call\(\s*["\']generate_image["\']\s*,\s*\{.*\}\s*\)\s*$',
    re.S
)

# --------------------------------------------------
# åˆ†æµå‡½å¼
# --------------------------------------------------
def on_generate():
    """
    æ¨¡å‹åˆ¤å®šè¦ç”Ÿæˆåœ–ç‰‡æ™‚åŸ·è¡Œ
    """
    print("è¦ç”¢åœ–")


def on_no_generate(reply: str):
    """
    æ¨¡å‹åˆ¤å®šä¸ç”Ÿæˆåœ–ç‰‡æ™‚åŸ·è¡Œ
    """
    print("ä¸å¿…è¦ç”¢åœ–")
    print("Assistant å›ç­”ï¼š", reply)

# --------------------------------------------------
# å»ºç«‹ ChatML Prompt
# --------------------------------------------------
def build_prompt(user_input: str) -> str:
    """
    å–®å›åˆæ¨ç† Promptï¼šåªåŒ…å« system + user + assistant é–‹é ­
    """
    prompt = (
        f"<|im_start|>system\n{SYSTEM_PROMPT}<|im_end|>\n"
        f"<|im_start|>user\n{user_input}<|im_end|>\n"
        "<|im_start|>assistant\n"
    )
    return prompt

# --------------------------------------------------
# åˆ¤æ–·æ˜¯å¦ç‚ºç”Ÿæˆå‘¼å«
# --------------------------------------------------
def is_generate(reply: str) -> bool:
    """
    åªæœ‰ç•¶å›è¦†å®Œå…¨æ˜¯ call(...) æ‰è¦–ç‚ºç”Ÿæˆæ„åœ–
    """
    return bool(CALL_FULLMATCH.fullmatch(reply.strip()))

# --------------------------------------------------
# ä¸»ç¨‹å¼
# --------------------------------------------------
def main():
    parser = argparse.ArgumentParser(
        description="Llama-3 Function-Calling Inference (Base+LoRA)"
    )
    parser.add_argument(
        "--base_model", default="unsloth/Llama-3.1-8B-unsloth-bnb-4bit",
        help="4-bit base æ¨¡å‹è·¯å¾‘æˆ– HF åç¨±"
    )
    parser.add_argument(
        "--adapter_dir", required=True,
        help="LoRA Î” æ¬Šé‡è³‡æ–™å¤¾ (save_method='lora')"
    )
    parser.add_argument(
        "--device", default="cuda",
        help="cuda / cpu / auto"
    )
    args = parser.parse_args()

    # è¼‰å…¥ 4-bit base æ¨¡å‹
    print("â–º è¼‰å…¥ 4-bit base æ¨¡å‹â€¦")
    bnb_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    base = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        quantization_config=bnb_cfg,
        device_map="auto" if args.device == "auto" else args.device,
    )
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)

    # å¥—ç”¨ LoRA adapter
    print("â–º å¥—ç”¨ LoRA adapterâ€¦")
    model = PeftModel.from_pretrained(base, args.adapter_dir)
    model.eval()

    eos_id = tokenizer.eos_token_id

    print("=== å–®å›åˆäº’å‹•æ¨¡å¼ (Ctrl+C é›¢é–‹) ===")
    while True:
        try:
            user_input = input("\nğŸ§‘â€ğŸ’» User > ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nBye!")
            break
        if not user_input:
            continue

        # çµ„ prompt
        prompt_str = build_prompt(user_input)
        inputs = tokenizer(prompt_str, return_tensors="pt").to(model.device)

        # æ¨ç†
        with torch.inference_mode():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=128,
                eos_token_id=eos_id,
                pad_token_id=eos_id,
            )[0]

        # åˆ‡å‡ºç”Ÿæˆéƒ¨åˆ†ä¸¦ decode
        prompt_len = inputs["input_ids"].shape[1]
        gen_ids = output_ids[prompt_len:]
        raw = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

        # ç§»é™¤å¯èƒ½æ®˜ç•™çš„ ChatML æ¨™è¨˜
        if "<|im_end|>" in raw:
            raw = raw.split("<|im_end|>")[0]
        if "<|im_start|>" in raw:
            raw = raw.split("<|im_start|>")[0].strip()

        # åš´æ ¼åˆ¤å®šæ˜¯å¦ç”Ÿæˆ
        if is_generate(raw):
            on_generate()
        else:
            on_no_generate(raw)

if __name__ == "__main__":
    main()
